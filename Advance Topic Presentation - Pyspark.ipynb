{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 652 - Advance Topic Presentation - PySpark\n",
    "\n",
    "*Presented By - Rahul Rathod and Yash Kapadia*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Spark?\n",
    "- **Apache Spark** is a fast, in-memory analytics system.\n",
    "- Spark has several high-level tools, including:\n",
    "  - **ML**: a machine learning library.\n",
    "  - **Spark Streaming**: enables high-throughput, fault-tolerant stream processing of live data streams.\n",
    "  - **Spark SQL**: runs SQL and HiveQL queries.\n",
    "  - **GraphX**: an API for graphs and graph-parallel computation.\n",
    "- Spark can be executed in two ways:\n",
    "  - Independent processes on a cluster.\n",
    "  - As a YARN application.  \n",
    "- PySpark is a Python API written in python to support Apache Spark. It is not a part of python standard library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is PySpark?\n",
    "PySpark is the Python API written in python to support Apache Spark. Apache Spark is a distributed framework that can handle Big Data analysis. Apache Spark is written in Scala and can be integrated with Python, Scala, Java, R, SQL  languages. Spark is basically a computational engine, that works with huge sets of data by processing them in parallel and batch systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Anaconda Navigator: Installing pyspark and py4j package\n",
    "- Install Java 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Spark 2.0+\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Spark 1.6 (RDDs)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SparkContext\n",
    "\n",
    "- The **SparkContext** object performs the following tasks:  \n",
    "\n",
    "  - It connects to the YARN ResourceManager and asks for resources on the Hadoop cluster,  \n",
    "  \n",
    "  - Starts executors on the worker nodes in the cluster that the ResourceManager allocated for the Spark application,  \n",
    "  \n",
    "  - Sends the application code to the executors,  \n",
    "  \n",
    "  - And finally, it sends tasks for the executors to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDDs\n",
    "- An RDD (resilient distributed dataset) is a fault-tolerant collection of elements that can be operated on in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating RDDs\n",
    "- Can create an initial RDD by applying a transformation to data on disk.  \n",
    "\n",
    "- Can create an initial RDD from a code object.  \n",
    "\n",
    "- Example ways to create an RDD in Spark:\n",
    "  - Use the `parallelize` operation to convert an existing code object into an RDD.\n",
    "  - Use `textFile` operation to convert a text file on HDFS into an RDD.\n",
    "  - Use `sequenceFile` operation to convert a binary file on HDFS into an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Simple Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myarray = list(range(1, 20))\n",
    "myarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelizing the Array - RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_array = sc.parallelize(myarray) # parallelize version of myarray\n",
    "dist_array # it doesn't do anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Operations\n",
    "- There are two types of operations that can be done on RDDs:\n",
    "  - **Transformations**: create a new dataset/RDD from an existing one.\n",
    "  - **Actions**: which return a value to the driver program after running a computation on the RDD.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Transformations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtract_values [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "large_values [11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "sub_values = dist_array.map(lambda x: x-1)\n",
    "print(\"subtract_values\", sub_values.collect())\n",
    "large_values = dist_array.filter(lambda y: y > 10)\n",
    "print(\"large_values\", large_values.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the type after transformation\n",
    "type(sub_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_values = dist_array.filter(lambda y: y > 10)\n",
    "large_values.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "- The problem with RDDs is that they do not have enough structure.  \n",
    "\n",
    "- They are harder to optimize and therefore slow.  \n",
    "\n",
    "- DataFrames aim at solving this by adding structure.  \n",
    "\n",
    "- A DataFrame is a distributed collection of data organized into named columns.  \n",
    "\n",
    "- Similar to Pandas DataFrames but distributed across the cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Pyspark Data Frame from Python Row Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# creating a Dataframe using python row objects\n",
    "\n",
    "raw_data = [Row(state='NY', month='JAN', orders=3),\n",
    "            Row(state='NJ', month='JAN', orders=4),\n",
    "            Row(state='NY', month='FEB', orders=5)\n",
    "           ]\n",
    "data_df = spark.createDataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|month|orders|state|\n",
      "+-----+------+-----+\n",
      "|  JAN|     3|   NY|\n",
      "|  JAN|     4|   NJ|\n",
      "|  FEB|     5|   NY|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing a dataframe\n",
    "# .show() will display 20 rows by default\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- orders: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can check the schema \n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing SQL like Operations on Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "# creating a dataframe for location\n",
    "locations_df = spark.createDataFrame([\n",
    "    Row(location_id = 'loc1', n_employees=3, state='NY'),\n",
    "    Row(location_id = 'loc2', n_employees=8, state='NY'),\n",
    "    Row(location_id = 'loc3', n_employees=3, state='PA'),\n",
    "    Row(location_id = 'loc4', n_employees=1, state='FL')    \n",
    "])\n",
    "\n",
    "# creating a dataframe for transcations\n",
    "transactions_df = spark.createDataFrame([\n",
    "    Row(transaction_id = 1, location_id = 'loc1', n_orders=2.),\n",
    "    Row(transaction_id = 2, location_id = 'loc1', n_orders=3.),\n",
    "    Row(transaction_id = 3, location_id = 'loc3', n_orders=5.),\n",
    "    Row(transaction_id = 4, location_id = 'loc5', n_orders=5.)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|location_id|n_employees|state|\n",
      "+-----------+-----------+-----+\n",
      "|       loc1|          3|   NY|\n",
      "|       loc2|          8|   NY|\n",
      "|       loc3|          3|   PA|\n",
      "|       loc4|          1|   FL|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------+\n",
      "|location_id|n_orders|transaction_id|\n",
      "+-----------+--------+--------------+\n",
      "|       loc1|     2.0|             1|\n",
      "|       loc1|     3.0|             2|\n",
      "|       loc3|     5.0|             3|\n",
      "|       loc5|     5.0|             4|\n",
      "+-----------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+------------------+----------------+\n",
      "|n_employees|location_id|state|n_employees_plus_1|more_than_5_empl|\n",
      "+-----------+-----------+-----+------------------+----------------+\n",
      "|          3|       loc1|   NY|                 4|           false|\n",
      "|          8|       loc2|   NY|                 9|            true|\n",
      "|          3|       loc3|   PA|                 4|           false|\n",
      "|          1|       loc4|   FL|                 2|           false|\n",
      "+-----------+-----------+-----+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding 1 to the n_employees column and checking for n_employees > 5\n",
    "locations_df.\\\n",
    "    select('n_employees',\n",
    "          'location_id',\n",
    "          'state',\n",
    "          (fn.col('n_employees') + 1).alias('n_employees_plus_1'),\n",
    "          (fn.col('n_employees') > 5).alias('more_than_5_empl')\n",
    "          ).\\\n",
    "    show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying functions like square root and exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+---------------------+\n",
      "| SQRT(n_employees)|n_employees|POWER(n_employees, 2)|\n",
      "+------------------+-----------+---------------------+\n",
      "|1.7320508075688772|          3|                  9.0|\n",
      "|2.8284271247461903|          8|                 64.0|\n",
      "|1.7320508075688772|          3|                  9.0|\n",
      "|               1.0|          1|                  1.0|\n",
      "+------------------+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Taking square root of number of employees and calculating the square of each value in n_employees column\n",
    "locations_df.\\\n",
    "    select(fn.sqrt('n_employees'),\n",
    "           'n_employees',\n",
    "           fn.pow(fn.col('n_employees'), fn.lit(2))\n",
    "          ).\\\n",
    "    show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|location_id|n_employees|state|\n",
      "+-----------+-----------+-----+\n",
      "|       loc3|          3|   PA|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering the Dataframe where number of employees greater than 2 and state is PA\n",
    "locations_df.where((fn.col('n_employees') > 2) & \\\n",
    "                  (fn.col('state') == 'PA')).\\\n",
    "         show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the two dataframes on location_id\n",
    "# default join is inner join\n",
    "new_df = locations_df.join(transactions_df, on='location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+--------+--------------+\n",
      "|location_id|n_employees|state|n_orders|transaction_id|\n",
      "+-----------+-----------+-----+--------+--------------+\n",
      "|       loc1|          3|   NY|     2.0|             1|\n",
      "|       loc1|          3|   NY|     3.0|             2|\n",
      "|       loc3|          3|   PA|     5.0|             3|\n",
      "+-----------+-----------+-----+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the execution plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [location_id#16, n_employees#17L, state#18, n_orders#23, transaction_id#24L]\n",
      "+- *(5) SortMergeJoin [location_id#16], [location_id#22], Inner\n",
      "   :- *(2) Sort [location_id#16 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(location_id#16, 200)\n",
      "   :     +- *(1) Filter isnotnull(location_id#16)\n",
      "   :        +- Scan ExistingRDD[location_id#16,n_employees#17L,state#18]\n",
      "   +- *(4) Sort [location_id#22 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(location_id#22, 200)\n",
      "         +- *(3) Filter isnotnull(location_id#22)\n",
      "            +- Scan ExistingRDD[location_id#22,n_orders#23,transaction_id#24L]\n"
     ]
    }
   ],
   "source": [
    "new_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataframe using spark.read.csv method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = spark.read.csv('Bank Salaries.csv', \n",
    "                             header=True, \n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee: integer (nullable = true)\n",
      " |-- EducLev: integer (nullable = true)\n",
      " |-- JobGrade: integer (nullable = true)\n",
      " |-- YrsExper: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- YrsPrior: integer (nullable = true)\n",
      " |-- PCJob: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the schema\n",
    "bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+--------+---+------+--------+-----+------+----+----+\n",
      "|Employee|EducLev|JobGrade|YrsExper|Age|Gender|YrsPrior|PCJob|Salary| _c9|_c10|\n",
      "+--------+-------+--------+--------+---+------+--------+-----+------+----+----+\n",
      "|       1|      3|       1|       3| 26|  Male|       1|   No| 32000|null|null|\n",
      "|       2|      1|       1|      14| 38|Female|       1|   No| 39100|null|null|\n",
      "|       3|      1|       1|      12| 35|Female|       0|   No| 33200|null|null|\n",
      "|       4|      2|       1|       8| 40|Female|       7|   No| 30600|null|null|\n",
      "|       5|      3|       1|       3| 28|  Male|       0|   No| 29000|null|null|\n",
      "|       6|      3|       1|       3| 24|Female|       0|   No| 30500|null|null|\n",
      "|       7|      3|       1|       4| 27|Female|       0|   No| 30000|null|null|\n",
      "|       8|      3|       1|       8| 33|  Male|       2|   No| 27000|null|null|\n",
      "|       9|      1|       1|       4| 62|Female|       0|   No| 34000|null|null|\n",
      "|      10|      3|       1|       9| 31|Female|       0|   No| 29500|null|null|\n",
      "|      11|      3|       1|       9| 34|Female|       2|   No| 26800|null|null|\n",
      "|      12|      2|       1|       8| 37|Female|       8|   No| 31300|null|null|\n",
      "|      13|      2|       1|       9| 37|Female|       0|   No| 31200|null|null|\n",
      "|      14|      2|       1|      10| 58|Female|       6|   No| 34700|null|null|\n",
      "|      15|      3|       1|       4| 33|Female|       0|   No| 30000|null|null|\n",
      "|      16|      3|       1|       3| 27|Female|       0|   No| 31000|null|null|\n",
      "|      17|      3|       1|       6| 30|Female|       0|   No| 27000|null|null|\n",
      "|      18|      2|       1|       8| 37|Female|       9|   No| 29600|null|null|\n",
      "|      19|      3|       1|       5| 44|Female|       6|   No| 32600|null|null|\n",
      "|      20|      2|       1|       4| 29|Female|       3|   No| 29600|null|null|\n",
      "+--------+-------+--------+--------+---+------+--------+-----+------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the first 20 rows\n",
    "bank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [3759.8973095338047,628.2390899498624,1129.697425428689]\n",
      "RMSE: 5999.123340\n",
      "R2: 0.714577\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "\n",
    "# Create a vector representation for features\n",
    "assembler = VectorAssembler(inputCols=['JobGrade', 'YrsExper', 'EducLev'],outputCol=\"features\")\n",
    "train_df = assembler.transform(bank_df)\n",
    "\n",
    "\n",
    "# Fit a linear regression model\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='Salary')\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "\n",
    "# Output statistics \n",
    "trainingSummary = lr_model.summary\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"R2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Pipelines using Pyspark for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipe_model = Pipeline(stages=[VectorAssembler(inputCols=['JobGrade', 'YrsExper', 'EducLev'],outputCol=\"feature\"),\n",
    "                             LinearRegression(featuresCol = 'feature', labelCol='Salary')]).fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipe_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_b279eca27973, LinearRegression_bebf6f420eb2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the stages of the pipeline\n",
    "pipe_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([3759.8973, 628.2391, 1129.6974])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the coefficients of the Linear Regression model from the pipeline\n",
    "pipe_model.stages[1].coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why problems can be solved using Pyspark?\n",
    "\n",
    "PySpark can significantly accelerate analysis by making it easy to combine local and distributed data transformation operations while keeping control of computing costs. In addition, the language helps data scientists to avoid always having to downsample large sets of data. For tasks such as building a recommendation system or training a machine-learning system, using PySpark is something to consider. It is important for you to take advantage of distributed processing can also make it easier to augment existing data sets with other types of data and the example it includes like combining share-price data with weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
